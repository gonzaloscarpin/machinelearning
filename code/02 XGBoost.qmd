---
title: "02 XGBoost"
format: html
---

# XGBoost Model for Cotton Lint Yield Prediction

This document implements a XGBoost regression model to predict cotton lint yield based on genotypic, management, and environmental factors. The workflow includes data preparation, model training with hyperparameter tuning, validation, and interpretation using Shapley values.

## 1. Environment Setup

First, we load all required packages for the analysis.

```{r setup}
#| message: false
#| warning: false

# Core modeling packages
library(tidymodels)     # Unified modeling framework
library(doParallel)     # For parallel processing
library(tidyverse)      # Data manipulation and visualization
library(xgboost)         # Fast implementation of XGboost

# Model interpretation packages
library(shapviz)        # For Shapley value visualization
library(fastshap)       # Fast computation of Shapley values

# Additional utilities
library(finetune)       # Advanced tuning methods
library(readxl)         # Excel file import
library(patchwork)      # Combine plots
tidymodels_prefer()
```

## 2. Data Import and Exploration

Let's import our dataset and examine its structure.

```{r data_import}
# Import cotton yield dataset
df <- read_excel("../data/data_example.xlsx")

# Display the first few rows of the data
df
```

### 2.1 Exploratory Data Analysis

Let's examine summary statistics to understand our data distribution.

```{r data_summary}
# Display summary statistics for all variables
df %>% 
  summary()
```

### 2.2 Data Preprocessing

We need to transform categorical variables to factors and remove variables not needed for modeling.

```{r data_wrangling}
# Convert character variables to factors and remove year and block
df_w <- df %>% 
  mutate_if(is.character, as.factor)

# View the transformed data
df_w
```

## 3. Machine Learning Workflow

### 3.1 Data Splitting

For robust model evaluation, we split the data into training (70%) and testing (30%) sets, stratified by the target variable.

```{r data_split}
# Setting seed to get reproducible results  
set.seed(27)

# Split data while preserving the distribution of the target variable
df_split <- initial_split(df, prop = 0.7, strata = "lintyield_kgha")
df_split
```

```{r extract_train_data}
# Extract the training dataset
df_train <- training(df_split)
df_train
```

```{r extract_test_data}
# Extract the testing dataset
df_test <- testing(df_split)
df_test
```

### 3.2 Visualizing Data Distribution

Let's verify that our training and testing sets have similar distributions of the target variable.

```{r visualize_data_distribution}
# Combine the data frames with dataset labels
plot_train <- df_train %>% mutate(Dataset = "Train")
plot_test <- df_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create a density plot to compare distributions
ggplot(data = combined_data) +
  geom_rug(aes(x = lintyield_kgha, fill = Dataset)) +
  geom_density(aes(x = lintyield_kgha, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Cotton Lint Yield (kg/ha)", 
       y = "Density", 
       fill = "Dataset",
       title = "Distribution of Lint Yield in Training and Testing Sets")
```
  
### 3.3 Feature Engineering

We define a recipe for data preprocessing, removing unnecessary columns.

```{r define_recipe}
# Create a recipe to preprocess the data
df_recipe <-
  # Defining predicted and predictor variables
  recipe(lintyield_kgha ~ ., data = df_train) %>%
  # Remove year and block variables as they shouldn't influence predictions
  step_rm(year, block) %>% 
  # Transform to dummy variables factor predictors
  step_dummy(all_factor_predictors())

# Display the recipe
df_recipe
```

```{r prepare_recipe}
# Prepare (train) the recipe on the training data
df_prep <- df_recipe %>% prep()
df_prep
```

## 4. Model Training

### 4.1 Model Specification

We define a XGBoost model with hyperparameters to be tuned.

```{r model_specification}
# Define the XGBoost model specification with parameters to tune
xgb_spec <- boost_tree(
  # Model complexity parameters
  trees = tune(),              # Number of trees (boosting iterations)
  tree_depth = tune(),         # Maximum depth of trees (controls complexity)
  min_n = tune(),              # Minimum number of data points in a node
  loss_reduction = tune(),     # Minimum loss reduction for further partition
  
  # Randomness/stochasticity parameters  
  sample_size = tune(),        # Fraction of data used in each tree
  mtry = tune(),               # Number of predictors randomly sampled at each split
  
  # Learning parameters
  learn_rate = tune()          # Step size shrinkage to prevent overfitting
) %>%
  set_engine("xgboost") %>%    # Use XGBoost as the implementation
  set_mode("regression")       # Specify regression problem

# Display model specification
xgb_spec
```

### 4.2 Hyperparameter Tuning

We'll use 5-fold cross-validation to find the optimal hyperparameters.

```{r create_cv_folds}
# Create 5-fold cross-validation resamples
set.seed(15)
resampling_foldcv <- vfold_cv(df_train, v = 5)
```


```{r}
# Create a Latin Hypercube sampling grid for efficient hyperparameter space exploration
# This is more efficient than a regular grid search when you have many parameters
xgb_grid <- grid_latin_hypercube(
  trees(),                            # Number of trees to try
  tree_depth(),                       # Tree depth values to try
  min_n(),                            # Minimum node size values to try
  loss_reduction(),                   # Loss reduction threshold values to try
  sample_size = sample_prop(),        # Data sampling proportion values to try
  finalize(mtry(), resampling_foldcv),# Number of predictors (finalized based on data)
  learn_rate(),                       # Learning rate values to try
  size = 100                          # Total number of parameter combinations to test
)

# Preview the first few parameter combinations
head(xgb_grid)
```



```{r tune_hyperparameters}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# Perform racing-based tuning with ANOVA to efficiently search parameter space
# Racing methods can dramatically reduce computation time by eliminating poorly
# performing parameter combinations early in the process
xgb_tuning_results <- tune_race_anova(
  object = xgb_spec,
  preprocessor = df_recipe,  # Use your preprocessing recipe
  resamples = resampling_foldcv,  # Cross-validation folds
  grid = xgb_grid,                # Grid of hyperparameters to try
  control = control_race(
    save_pred = TRUE,             # Save predictions for later analysis
    parallel_over = "everything", # Parallelize everything possible
    verbose = TRUE                # Show progress
  )
)

# Stop parallel processing
stopImplicitCluster()

# Examine tuning results
xgb_tuning_results
```

```{r visualize_tuning_results}
# Visualize the hyperparameter tuning results
autoplot(xgb_tuning_results)
```

### 4.3 Selecting Optimal Hyperparameters

We'll evaluate different strategies for selecting the best hyperparameters.

```{r select_by_rmse}
# Selection methods based on RMSE
# Method 1: Select model within 2% of the best RMSE, but with fewer trees
best_rmse <- xgb_tuning_results %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

# Method 2: Simply select the model with the lowest RMSE
best_rmse_2 <- xgb_tuning_results %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

# Method 3: Select model within one standard error of the best RMSE
best_rmse_3 <- xgb_tuning_results %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")
```

```{r select_by_r2}
# Selection methods based on R² (coefficient of determination)
# Method 1: Select model within 2% of the best R², but with fewer trees
best_r2 <- xgb_tuning_results %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

# Method 2: Simply select the model with the highest R²
best_r2_2 <- xgb_tuning_results %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

# Method 3: Select model within one standard error of the best R²
best_r2_3 <- xgb_tuning_results %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")
```

```{r combine_hyperparameters}
# Combine all candidate hyperparameter sets for comparison
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

### 4.4 Comparing Hyperparameter Performance

We evaluate each set of hyperparameters on our validation data to select the best.

```{r hyperparameter_comparison_function}
# Function to compare the performance of different hyperparameter sets
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = df_recipe,
  split = df_split
)

# Display results
print(results)
```

### 4.5 Final Model Specification

Using the best hyperparameters, we specify our final model.

```{r final_model_specification}
# Create the final model specification with the best hyperparameters
# Note: Use the source identified as best from the results above
final_spec  <- boost_tree(
  trees = best_r2$trees,
  tree_depth = best_r2$tree_depth, 
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,                     
  sample_size = best_r2$sample_size, 
  mtry = best_r2$mtry,           
  learn_rate = best_r2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

# Display the final specification  
final_spec
```

## 5. Model Validation

Now we fit the model on the training data and evaluate on the test data.

```{r final_model_fit}
# Fit the final model and predict on test data
set.seed(10)
final_fit <- last_fit(
  final_spec,
  df_recipe,
  split = df_split
)

# Display predictions on test data
final_fit %>%
  collect_predictions()
```

### 5.1 Test Set Performance

Let's examine how well our model performs on the unseen test data.

```{r test_metrics}
# Calculate performance metrics on the test set
final_fit %>%
  collect_metrics()
```

### 5.2 Training Set Performance

For comparison, we also check performance on the training data to assess potential overfitting.

```{r train_metrics}
# RMSE on training data
train_metrics <- final_spec %>%
  fit(lintyield_kgha ~ .,
      data = bake(df_prep, df_train)) %>%
  augment(new_data = bake(df_prep, df_train))

# Calculate and combine RMSE and R² on training data
train_metrics %>% 
  rmse(lintyield_kgha, .pred) %>%
  bind_rows(
    train_metrics %>% 
    rsq(lintyield_kgha, .pred)
  )
```

### 5.3 Prediction Visualization

Let's visualize how well our predictions match the observed values.

```{r predicted_vs_observed}
# Create scatter plot of predicted vs. observed values
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(
    obs = lintyield_kgha,
    pred = .pred,
    print_eq = TRUE,
    print_metrics = TRUE,
    metrics_list = c("R2", "RMSE"),
    # Customize metrics position
    position_metrics = c(x=700, y=1850),
    position_eq = c(x=700, y=1930)
  )

# Display the plot
plot
```

### 5.4 Comprehensive Performance Metrics

Let's calculate a comprehensive set of performance metrics.

```{r comprehensive_metrics}
# Get predictions on test set
test_results <- final_fit %>% 
  collect_predictions()

# Calculate multiple performance metrics
metrics <- test_results %>% 
  summarise(
    RMSE = rmse_vec(truth = lintyield_kgha, estimate = .pred),
    R2 = rsq_vec(lintyield_kgha, .pred),
    Correlation = cor(.pred, lintyield_kgha),
    SD = sd(.pred)  # Standard deviation of predictions
  )

# Display the metrics
print(metrics)
```

## 6. Model Interpretation with Shapley Values

Shapley values help us understand the contribution of each feature to individual predictions.

### 6.1 Preparing for Shapley Analysis

```{r prepare_shapley}
# Extract the fitted model
set.seed(27)
fitted_model <- extract_fit_parsnip(final_fit$.workflow[[1]])

# Define a prediction function for Shapley calculations
predict_function <- function(model, newdata) {
  # For XGBoost specifically, ensure data is numeric
  if(inherits(model$fit, "xgb.Booster")) {
    newdata <- as.matrix(newdata)
  }
  predict(model, new_data = newdata)$.pred
}

# Prepare test data for Shapley analysis (without the target variable)
X <- bake(df_prep, df_test) %>%
       select(-lintyield_kgha)
```

### 6.2 Calculating Shapley Values
```{r}
# Note: Consider increasing nsim for more robust results in production
# We use parallel processing to speed up calculations
set.seed(27)

system.time({ 
  shap_values <- fastshap::explain(
    fitted_model, 
    X = X, 
    pred_wrapper = predict_function,
    adjust = TRUE,
    parallel = F,
    nsim = 50  # Number of Monte Carlo simulations
  )
})
```

```{r create_shapviz}
# Create a shapviz object for visualization
shapviz_obj <- shapviz(shap_values, X = X)
```

### 6.3 Feature Importance Based on Shapley Values

```{r shapley_importance}
# Create a bar plot of feature importance
shap_values_fi <- sv_importance(shapviz_obj, show_numbers = TRUE)
shap_values_fi
```

### 6.4 Detailed Shapley Value Distribution

```{r shapley_beeswarm}
# Create a beeswarm plot showing the distribution of Shapley values
shap_values_fi_bee <- sv_importance(shapviz_obj, kind = "bee")
shap_values_fi_bee
```

### 6.5 Individual Prediction Analysis

Let's examine how features contribute to specific predictions.

```{r individual_prediction_analysis}
# Show predictions
final_fit %>% 
  collect_predictions()

# Waterfall plot for the first observation
sv_waterfall(shapviz_obj, row_id = 1)

# Force plot for the first observation
sv_force(shapviz_obj, row_id = 1)
```

### 6.6 Combined Visualization

Let's combine our key visualizations into one figure.

```{r combined_plots, fig.height=11, fig.width=8}
# Combine predicted vs observed plot with Shapley importance visualizations
together <- plot / shap_values_fi / shap_values_fi_bee

# Display the combined figure
together
```

## 7. Conclusion

This analysis demonstrates the application of XGBoost for predicting cotton lint yield. The model achieves good predictive performance, and Shapley values provide insights into the contribution of different features to the predictions.