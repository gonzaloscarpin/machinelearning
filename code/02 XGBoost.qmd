---
title: "Random forest"
format: htML_Grade
---

# Setup  
```{r}
#| message: false
#| warning: false

library(doParallel)
library(tidymodels)
library(tidyverse)
library(vip)
library(xgboost)
library(sf)
library(finetune)
```

# 1) M + W + S + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% 
  dplyr::filter(grade_n > 65)
  

df
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")
```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
hyperparameters_df
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```


Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2$trees,
  tree_depth = best_r2$tree_depth, 
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,                     
  sample_size = best_r2$sample_size, 
  mtry = best_r2$mtry,           
  learn_rate = best_r2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```

```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =72))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+S+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+S+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 2) M + W + S 

# Yield
```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60#,
               # NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")
```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```
```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse_2$trees,
  tree_depth = best_rmse_2$tree_depth, 
  min_n = best_rmse_2$min_n,
  loss_reduction = best_rmse_2$loss_reduction,                     
  sample_size = best_rmse_2$sample_size, 
  mtry = best_rmse_2$mtry,           
  learn_rate = best_rmse_2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=73),
                        position_eq = c(x=75, y =70.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+S.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+S_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```


# 3) M + W + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                stage_duration_emergence:stage_vp_total,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% 
  dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2$trees,
  tree_depth = best_r2$tree_depth, 
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,                     
  sample_size = best_r2$sample_size, 
  mtry = best_r2$mtry,           
  learn_rate = best_r2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```

```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=77,y=72.5),
                        position_eq = c(x=77, y =73.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 4) M + S + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse_2$trees,
  tree_depth = best_rmse_2$tree_depth, 
  min_n = best_rmse_2$min_n,
  loss_reduction = best_rmse_2$loss_reduction,                     
  sample_size = best_rmse_2$sample_size, 
  mtry = best_rmse_2$mtry,           
  learn_rate = best_rmse_2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=76,y=70),
                        position_eq = c(x=76, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+S+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+S+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 5) W + S + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```


Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2_3$trees,
  tree_depth = best_r2_3$tree_depth, 
  min_n = best_r2_3$min_n,
  loss_reduction = best_r2_3$loss_reduction,                     
  sample_size = best_r2_3$sample_size, 
  mtry = best_r2_3$mtry,           
  learn_rate = best_r2_3$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/W+S+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade_Grade/01_XGBoost/W+S+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 6) M + W

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                stage_duration_emergence:stage_vp_total#,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                #NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse_3$trees,
  tree_depth = best_rmse_3$tree_depth, 
  min_n = best_rmse_3$min_n,
  loss_reduction = best_rmse_3$loss_reduction,                     
  sample_size = best_rmse_3$sample_size, 
  mtry = best_rmse_3$mtry,           
  learn_rate = best_rmse_3$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade_Grade/01_XGBoost/M+W.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/M+W_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 7) M + S

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60#,
               # NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2$trees,
  tree_depth = best_r2$tree_depth, 
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,                     
  sample_size = best_r2$sample_size, 
  mtry = best_r2$mtry,           
  learn_rate = best_r2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/M+S.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/M+S_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 8) M + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2$trees,
  tree_depth = best_r2$tree_depth, 
  min_n = best_r2$min_n,
  loss_reduction = best_r2$loss_reduction,                     
  sample_size = best_r2$sample_size, 
  mtry = best_r2$mtry,           
  learn_rate = best_r2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=78,y=74.5),
                        position_eq = c(x=78, y =75.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/M+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/M+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 9) W + S

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60#,
                #NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2_3$trees,
  tree_depth = best_r2_3$tree_depth, 
  min_n = best_r2_3$min_n,
  loss_reduction = best_r2_3$loss_reduction,                     
  sample_size = best_r2_3$sample_size, 
  mtry = best_r2_3$mtry,           
  learn_rate = best_r2_3$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.8),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/W+S.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/W+S_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 10) W + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                stage_duration_emergence:stage_vp_total,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```
```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse_2$trees,
  tree_depth = best_rmse_2$tree_depth, 
  min_n = best_rmse_2$min_n,
  loss_reduction = best_rmse_2$loss_reduction,                     
  sample_size = best_rmse_2$sample_size, 
  mtry = best_rmse_2$mtry,           
  learn_rate = best_rmse_2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=77,y=73.5),
                        position_eq = c(x=77, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/W+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/W+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 11) S + O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse$trees,
  tree_depth = best_rmse$tree_depth, 
  min_n = best_rmse$min_n,
  loss_reduction = best_rmse$loss_reduction,                     
  sample_size = best_rmse$sample_size, 
  mtry = best_rmse$mtry,           
  learn_rate = best_rmse$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/S+O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/S+O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 12) M 

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, planting_date:variety, 
                row_pattern:saved_seed, 
                lat, lon#, 
                #stage_duration_emergence:stage_vp_total,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                #NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```


Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse_2$trees,
  tree_depth = best_rmse_2$tree_depth, 
  min_n = best_rmse_2$min_n,
  loss_reduction = best_rmse_2$loss_reduction,                     
  sample_size = best_rmse_2$sample_size, 
  mtry = best_rmse_2$mtry,           
  learn_rate = best_rmse_2$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/M.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/M_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 13) W

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                stage_duration_emergence:stage_vp_total#,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                #NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_r2_3$trees,
  tree_depth = best_r2_3$tree_depth, 
  min_n = best_r2_3$min_n,
  loss_reduction = best_r2_3$loss_reduction,                     
  sample_size = best_r2_3$sample_size, 
  mtry = best_r2_3$mtry,           
  learn_rate = best_r2_3$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.5),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/W.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/W_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 14) S

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                river_basin, soil_series,
                soil_clay_0_5:soil_theta_s_30_60#,
                #NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

et's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse$trees,
  tree_depth = best_rmse$tree_depth, 
  min_n = best_rmse$min_n,
  loss_reduction = best_rmse$loss_reduction,                     
  sample_size = best_rmse$sample_size, 
  mtry = best_rmse$mtry,           
  learn_rate = best_rmse$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=70.8),
                        position_eq = c(x=75, y =71.5))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/S.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/S_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

# 15) O

# Yield


```{r }
df <- read_sf("../data/data_complete.geojson") %>% 
  st_drop_geometry() %>% 
  as.data.frame() %>% 
  dplyr::select(planting_date, digger_date, growing_season, variety, river_basin, soil_series, row_pattern, 
                irrigation, saved_seed, 
                tillage_method=tillage_method_simplify,
                rotation = rotation_simplify_years_wo_peanut, seeding_rate, 
                grade_n, 
                alt, stage_duration_emergence:elevation,
                lat= lat.y, lon = long
                ) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(rotation = as.factor(rotation)) %>% 
  dplyr::select(-(c(alt, rotation, seeding_rate, tillage_method))) %>% 
  dplyr::select(!(c(soil_alpha_0_5:soil_bd_30_60, 
                    soil_hb_0_5:soil_hb_30_60, 
                    soil_lambda_0_5:soil_n_30_60,
                    soil_theta_r_0_5:soil_theta_r_30_60
                    ))) %>%
  #dplyr::select(-(c("lat", "lon"))) %>% 
  
  dplyr::select(-(c(SCI:NGRDI))) %>% 
  dplyr::select(grade_n, 
                #planting_date:variety, 
                #row_pattern:saved_seed, 
                #lat, lon, 
                #stage_duration_emergence:stage_vp_total,
                #river_basin, soil_series,
                #soil_clay_0_5:soil_theta_s_30_60,
                NDVI:elevation
                ) %>% 
  #dplyr::select(-(c(NDVI:GNDVI, elevation))) %>% 
  drop_na() %>% dplyr::filter(grade_n > 65) 

df

# df <- read_csv("../data/df.csv") %>%
#   distinct(n_field, .keep_all = T) %>% 
#   dplyr::select(river_basin, variety, irrigation, saved_seed, soil_series, grade_n) %>% 
#   drop_na() %>% dplyr::filter(grade_n > 65)
```

# ML_Grade workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(27)
weather_split <- initial_split(df, 
                               prop = .7,
                               strata = "grade_n")

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Now, we put our **test set** aside and continue with our **train set** for training.  
```{r}
# Combine the data frames
plot_train <- weather_train %>% mutate(Dataset = "Train")
plot_test <- weather_test %>% mutate(Dataset = "Test")
combined_data <- bind_rows(plot_train, plot_test)

# Create the density plot
ggplot(data = combined_data) +
  geom_density(aes(x = grade_n, fill = Dataset), alpha = 0.5) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Yield (kg/ha)", y = "Density", fill = "Dataset")
```
  
### b. Data processing  
> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step  

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(grade_n ~ .,
         data = weather_train) %>%
  #step_other(variety, threshold = 0.05) %>%
  #step_other(saved_seed, threshold = 0.05) %>%
  #step_rm() %>% 
  step_dummy(all_factor_predictors()) #%>% 
  #step_naomit(all_predictors(), all_outcomes()) #%>% 
  #step_normalize(all_predictors()) %>% 
  #step_center(all_predictors()) %>% 
  #step_scale(all_predictors())

weather_recipe

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}
set.seed(123)
xgb_spec  <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(15)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv
```

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), resampling_foldcv),
  learn_rate(),
  size = 100
)
```


```{r rf_grid_result}
set.seed(45)
library(doParallel)
registerDoParallel(cores = parallel::detectCores() - 1)

# rf_grid_result <- tune_sim_anneal(object = rf_spec,
#                      preprocessor = weather_recipe,
#                      resamples = resampling_foldcv,
#                     #param_info = rf_param,
#                     iter = 100
#                      )
# 
rf_grid_result <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster()

rf_grid_result$.metrics[[5]]
```

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 2
                     )%>% 
  mutate(source = "best_rmse")

best_rmse_2 <- rf_grid_result %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse_2")

best_rmse_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rmse",
                        trees
                        )%>% 
  mutate(source = "best_rmse_3")

best_rmse
best_rmse_2
best_rmse_3

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 2
                     ) %>% 
  mutate(source = "best_r2")

best_r2_2 <- rf_grid_result %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2_2")

best_r2_3 <- rf_grid_result %>% 
  select_by_one_std_err(metric = "rsq",
                        trees
                        ) %>%
  mutate(source = "best_r2_3")

best_r2
best_r2_2
best_r2_3
```


```{r comparing values}
hyperparameters_df <- best_rmse %>% 
  bind_rows(best_rmse_2, best_rmse_3, best_r2, best_r2_2, best_r2_3)
```

```{r}
# Function to find the best model
compare_hyperparameters <- function(params_df, recipe, split) {
  # Create empty tibble to store results
  results <- tibble(
    mtry = numeric(),
    trees = numeric(),
    min_n = numeric(),
    tree_depth = numeric(),
    learn_rate = numeric(),
    loss_reduction = numeric(),
    sample_size = numeric(),
    rmse = numeric(),
    source = character()
  )
  
  # Loop through each row of parameters
  for(i in 1:nrow(params_df)) {
    # Extract current parameters
    current_params <- params_df[i, ]
    
    # Create model specification
    set.seed(10)
    current_spec <- boost_tree(
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size,
      ) %>%
      set_engine("xgboost") %>%
      set_mode('regression')
    
    # Fit model and collect metrics
    current_fit <- last_fit(current_spec, 
                            recipe, 
                            split = split)
    current_metrics <- current_fit %>% 
      collect_metrics()
    
    # Extract RMSE value
    current_rmse <- current_metrics %>% 
      filter(.metric == "rmse") %>% 
      pull(.estimate)
    
    # Store results
    results <- results %>% add_row(
      source = current_params$source,
      rmse = current_rmse,
      mtry = current_params$mtry,
      trees = current_params$trees,
      min_n = current_params$min_n,
      tree_depth = current_params$tree_depth,
      learn_rate = current_params$learn_rate,
      loss_reduction = current_params$loss_reduction,
      sample_size = current_params$sample_size
    )
  }
  
  # Find the best combination
  best_params <- results %>% 
    arrange(rmse) %>% 
    dplyr::slice_head(n = 1)
  
  return(list(
    all_results = results,
    best_combination = best_params
  ))
}

# Example usage
results <- compare_hyperparameters(
  params_df = hyperparameters_df,
  recipe = weather_recipe,
  split = weather_split
)
print(results)
```

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec  <- boost_tree(
  trees = best_rmse$trees,
  tree_depth = best_rmse$tree_depth, 
  min_n = best_rmse$min_n,
  loss_reduction = best_rmse$loss_reduction,                     
  sample_size = best_rmse$sample_size, 
  mtry = best_rmse$mtry,           
  learn_rate = best_rmse$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(grade_n ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(grade_n, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(grade_n ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(grade_n, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = grade_n,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 8000)) +
  scale_y_continuous(limits = c(0, 8000)) 

```


```{r}
plot <- final_fit %>%
  collect_predictions() %>%
  metrica::scatter_plot(obs = grade_n,
                        pred = .pred,
                        print_eq = T,
                        print_metrics = T,
                        metrics_list = c("R2", "RMSE"),
                      # Customize metrics position
                        position_metrics = c(x=75,y=73.5),
                        position_eq = c(x=75, y =74))

plot
```


```{r}
ggsave(plot = plot,
       filename = "../output/ML_Grade/01_XGBoost/O.png",
       width = 5,
       height = 5
       )
```

Variable importance: 

```{r}
set.seed(10)
plot_2 <- final_spec %>%
  fit(grade_n ~ .,
         data = bake(weather_prep, df)) %>%
    vip::vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  dplyr::slice_head(n = 10) %>% 
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

plot_2
```


```{r}
ggsave(plot = plot_2,
       filename = "../output/ML_Grade/01_XGBoost/O_2.png",
       width = 5,
       height = 5,
       bg = "white"
       )  
```

